{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPOiajzJyIWU9shvfFh/P1x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aparnaashok2125/Text-Summarization-using-LLMs-T5-Transformer-Model-with-Python/blob/main/Text_Summarization_using_LLMs_(T5_Transformer_Model).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Summarization Model using T5 Transformer Model (LLM)\n",
        "# ---------------------------------------\n",
        "# Step-by-step implementation using Hugging Face Transformers"
      ],
      "metadata": {
        "id": "QeyhDAPxpNZO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A text summarization model takes a long text and creates a shorter version while preserving its main points. It works by extracting key sentences directly (extractive summarization) or rephrasing the content into a shorter form (abstractive summarization).**\n",
        "\n",
        "**To build a text summarization model, first, we need to choose a pre-trained language model like T5. Then, we need to tokenize the input text, which converts it into a format the model can process. The next step will be to use the model to generate a summary by specifying parameters like maximum length and beam search for better results. The final step will be to decode the generated tokens back into readable text and adjust parameters to improve the summary quality.**"
      ],
      "metadata": {
        "id": "Km3oAkwGuMyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select a Suitable LLM**\n",
        "\n",
        "*Choose a pre-trained model designed for text generation tasks. The T5 model (Text-to-Text Transfer Transformer) by Google is one such model that is effective for various text-based tasks like translation, question-answering, and summarization.*"
      ],
      "metadata": {
        "id": "Tt8qnR7jt_FE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Required Libraries"
      ],
      "metadata": {
        "id": "_QvO9eZxt6ge"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Install the transformers library by Hugging Face. It provides easy access to various pre-trained models and tokenizers. If you are using Google Colab, you will find it pre-installed in the Colab environment. To install it on your local machine, run the command mentioned below on your terminal or command prompt :*"
      ],
      "metadata": {
        "id": "qoASptuctkPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers"
      ],
      "metadata": {
        "id": "8tNP-8j8txlb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Now, we need to import the T5Tokenizer and T5ForConditionalGeneration classes from the transformers library. Select a model like t5-small, t5-base, or t5-large based on the requirement and computational capacity:*"
      ],
      "metadata": {
        "id": "rg804iHktFtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained T5 model and tokenizer\n",
        "# Now, we need to import the T5Tokenizer and T5ForConditionalGeneration classes from the transformers library. Select a model like t5-small, t5-base, or t5-large based on the requirement and computational capacity:\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "model_name = \"t5-small\"  # can be 't5-base' or 't5-large' based on resources\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "dDffhCdlpR1u"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*To select between t5-small, t5-base, or t5-large, consider your computational resources and accuracy needs. t5-small is faster and requires less memory, which makes it suitable for quick tasks or limited hardware. t5-base offers a balance between speed and performance, ideal for general use. t5-large provides the highest accuracy but needs more memory and processing power, which makes it better for scenarios where performance is more important than speed.*"
      ],
      "metadata": {
        "id": "TTLZuExcsiU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text to summarize\n",
        "# Next, we need to define the text that needs to be summarized. The text should be prefixed with the keyword “summarize:” for the T5 model to recognize the task properly:\n",
        "text = \"\"\"\n",
        "The COVID-19 pandemic has brought unprecedented challenges to the global economy.\n",
        "Governments worldwide have implemented strict lockdowns and social distancing measures\n",
        "to contain the virus spread. Many industries have been severely affected, leading to\n",
        "widespread job losses and financial instability across countries.\n",
        "\"\"\"\n",
        "# Prepare the text for the T5 model by adding the \"summarize:\" prefix\n",
        "input_text = \"summarize: \" + text"
      ],
      "metadata": {
        "id": "B_lIjJmcpRyT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The “summarize:” prefix for the T5 model is necessary because T5 is a “text-to-text” model that needs to understand what task it should perform (e.g., summarization, translation, or question-answering). The prefix helps the model identify that it should generate a summary of the input text. Without this instruction, the model might not produce the intended summarization output.*"
      ],
      "metadata": {
        "id": "MJdGl50MsYXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the input text\n",
        "# The next step is tokenization. Tokenization is the process of converting text into a sequence of integers that represent the model’s vocabulary. The max_length parameter helps manage large texts by truncating or limiting the input size:\n",
        "\n",
        "input_ids = tokenizer.encode(input_text,\n",
        "                             return_tensors=\"pt\", # returns PyTorch tensors\n",
        "                             max_length=512,     # limit input size\n",
        "                             truncation=True)    # truncate long inputs"
      ],
      "metadata": {
        "id": "ITKqf5jvpZFi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Now, use the generate method to produce the summary. Important parameters include:*\n",
        "\n",
        "\n",
        "*   max_length: The maximum number of tokens in the output.\n",
        "*   num_beams: The number of beams for beam search (higher values improve results but increase computation).\n",
        "*   length_penalty: Adjusts the length of the summary (penalizes lengthy outputs).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WFivUb17r5LM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the summary\n",
        "\n",
        "summary_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,   # maximum length of the summary\n",
        "    num_beams=4,     # beam search for better results\n",
        "    length_penalty=2.0,  # length penalty to avoid lengthy summaries\n",
        "    early_stopping=True\n",
        ")"
      ],
      "metadata": {
        "id": "qO9r00a5pY5k"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDdqDNZkoTnV",
        "outputId": "e9637a23-5a13-4c14-f521-b837aad9c114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: governments have implemented strict lockdowns and social distancing measures. many industries have been severely affected, leading to job losses and financial instability across countries.\n"
          ]
        }
      ],
      "source": [
        "# Decode and display the summary\n",
        "# The final step is to decode the generated summary using the tokenizer to convert the tokens back to readable text:\n",
        "\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(\"Summary:\", summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Summary :***\n",
        "\n",
        "*So, to build a text summarization model, first, we need to choose a pre-trained language model like T5. Then, we need to tokenize the input text, which converts it into a format the model can process. The next step will be to use the model to generate a summary by specifying parameters like maximum length and beam search for better results. The final step will be to decode the generated tokens back into readable text and adjust parameters to improve the summary quality.*"
      ],
      "metadata": {
        "id": "jDuFDLeXrSkc"
      }
    }
  ]
}